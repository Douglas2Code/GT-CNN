{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn import init\n",
    "from torch.utils.data import *\n",
    "\n",
    "from CNN import *\n",
    "from GradCAMUtils import *\n",
    "from Utils import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "\n",
    "from fastai import *\n",
    "from fastai.text import *\n",
    "from fastai.vision import *\n",
    "from fastai.imports import *\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import umap\n",
    "\n",
    "from matplotlib.widgets import Button\n",
    "from matplotlib.widgets import TextBox\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import genextreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Constants\"\"\"\n",
    "# sequence length indicate the maximum length for all of the sequnence 626/798\n",
    "SEQUENCE_LENGTH = 798\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "vocab = {'C': [0,0,1], 'H': [0,1,0], 'E': [1,0,0], '-':[0,0,0]}\n",
    "\n",
    "sns.set(rc={'figure.figsize':(15,15)})\n",
    "\n",
    "model_path = Path(\"./Models/\")\n",
    "path = Path(\"./Datasets/\")\n",
    "\n",
    "#show all lines and columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Transform the labels from String to Integer via LabelEncoder\n",
    "le_fold = preprocessing.LabelEncoder()\n",
    "le_fam = preprocessing.LabelEncoder()\n",
    "\n",
    "# torch.cuda.set_device()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "cuda_gpu = torch.cuda.is_available()   #check if gpu is avaliable\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multitask dataset overwrite of Dataset\n",
    "class MultitaskDataset(Dataset):\n",
    "    \"`Dataset` for joint single and multi-label image classification.\"\n",
    "    def __init__(self, data, labels_fold, labels_fam, paddings, cuda = True):   \n",
    "        self.data = torch.FloatTensor(data.float())\n",
    "        self.y_fam = torch.FloatTensor(labels_fam.float())\n",
    "        self.y_fold = torch.FloatTensor(labels_fold.float())\n",
    "        self.paddings = torch.FloatTensor(798-2*paddings.float())\n",
    "        \n",
    "        self.cuda = cuda\n",
    "    \n",
    "    def __len__(self): return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,i:int): \n",
    "        if self.cuda:\n",
    "            return torch.FloatTensor(self.data[i]).float().cuda(), torch.FloatTensor([self.y_fold[i], self.y_fam[i]]).float().cuda(),  self.paddings[i].cuda()\n",
    "        else:\n",
    "            return torch.FloatTensor(self.data[i]).float(), torch.FloatTensor([self.y_fold[i], self.y_fam[i]]).float(), self.paddings[i]\n",
    "\n",
    "# a helper function to load the data into custom dataset\n",
    "def Dataset_Loader(df, le_fam, le_fold, vocab, BATCH_SIZE, cuda = True):\n",
    "    x_train = torch.LongTensor(Map_Tokens(df.q3seqTokens, vocab))\n",
    "    y_train_fold = torch.LongTensor(le_fold.fit_transform(df[\"fold\"].values.ravel()))\n",
    "    y_train_fam = torch.LongTensor(le_fam.fit_transform(df[\"family\"].values.ravel()))\n",
    "    paddings = torch.LongTensor(df[\"paddings\"].values.ravel())\n",
    "    \n",
    "    ds = MultitaskDataset(x_train, y_train_fold, y_train_fam, paddings, cuda)\n",
    "    dl = DataLoader(\n",
    "        ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        pin_memory=False)\n",
    "    return ds, dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained final model\n",
    "model = pickle.load(open(\"../PretrainedModels/CNNAttention.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster definition\n",
    "GTA_0 = [\"GT14-A\", \"GT16-A\", \"GT2-A\",\"GT25-A\",\"GT45-A\", \"GT49-A\", \"GT60-A\"]\n",
    "GTA_1 = [\"GT15-A\",\"GT17-A\",\"GT31-A\",\"GT34-A\",\"GT43-A\",\"GT6-A\",\"GT62-A\",\"GT67-A\",\"GT7-A\",\"GT77-A\"]\n",
    "GTB_0 = [\"GT1-B\",\"GT10-B\",\"GT20-B\",\"GT28-B\",\"GT37-B\",\"GT38-B\",\"GT4-B\",\"GT47-B\",\"GT5-B\",\"GT63-B\",\"GT72-B\",\"GT79-B\",\"GT9-B\",\"GT90-B\",\"GT93-B\"]\n",
    "GTB_1 = [\"GT23-B\",\"GT3-B\",\"GT35-B\",\"GT41-B\"]\n",
    "GTB_1 = [\"GT104-B\",\"GT30-B\",\"GT70-B\"]\n",
    "GTC_0 = [\"GT39-C\",\"GT57-C\",\"GT66-C\"]\n",
    "GTC_1 = [\"GT50-C\",\"GT58-C\",\"GT87-C\"]\n",
    "GTC_2 = [\"GT22-C\",\"GT83-C\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19912 2490 2490\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "df = pd.read_csv(\"../Datasets/gt_training.non_augmented.csv\")\n",
    "\n",
    "df_cluter = df_large.loc[df_large['family'].isin(GTA_0)]\n",
    "\n",
    "train_df, val_df = Train_Test_Val_split(df_cluter, test_size=0.05, shuffle=False)\n",
    "test_df = df_large.loc[df_large.fold==\"A\"].loc[~df_large['family'].isin(GTA_0)]\n",
    "\n",
    "df_gtu = pd.read_csv(\"../Datasets/...\")\n",
    "\n",
    "Train_ds, Train_dl = Dataset_Loader(train_df, le_fam, le_fold, vocab, BATCH_SIZE=20, cuda = cuda_gpu)\n",
    "\n",
    "Val_ds, Val_dl = Dataset_Loader(val_df, le_fam, le_fold, vocab, BATCH_SIZE=20, cuda = cuda_gpu)\n",
    "\n",
    "Test_ds, Test_dl = Dataset_Loader(test_df, le_fam, le_fold, vocab, BATCH_SIZE=20, cuda = cuda_gpu)\n",
    "\n",
    "Val_u_ds, Val_u_dl = Dataset_Loader(df_gtu, le_fam, le_fold, vocab, BATCH_SIZE=20, cuda = cuda_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A = df.loc[df[\"fold\"]==\"A\"]\n",
    "df_B = df.loc[df[\"fold\"]==\"B\"]\n",
    "df_C = df.loc[df[\"fold\"]==\"C\"]\n",
    "df_Lyso = df.loc[df[\"fold\"]==\"lyso\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(autoencoder, self).__init__()\n",
    "        # pretrained cnn model\n",
    "        self.encoder = model\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "        # decoder\n",
    "        \n",
    "        self.trans1 = nn.ConvTranspose1d(in_channels=512, \n",
    "                                         out_channels=512, \n",
    "                                         kernel_size=15, \n",
    "                                         stride=15, \n",
    "                                         dilation=2, \n",
    "                                         padding=3)\n",
    "        \n",
    "        self.relu1 = torch.nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.in1 = nn.InstanceNorm1d(512)\n",
    "        \n",
    "        self.trans2 = nn.ConvTranspose1d(in_channels=512, \n",
    "                                         out_channels=512, \n",
    "                                         kernel_size=7, \n",
    "                                         stride=7,\n",
    "                                         dilation=2, \n",
    "                                         padding=1, \n",
    "                                         output_padding=1)\n",
    "\n",
    "        self.relu2 = torch.nn.ReLU(inplace=True)   \n",
    "        \n",
    "        self.in2 = nn.InstanceNorm1d(512)\n",
    "\n",
    "        self.trans3 = nn.ConvTranspose1d(in_channels=512, \n",
    "                                         out_channels=3, \n",
    "                                         kernel_size=3, \n",
    "                                         stride=1) \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        _, _, x, _ = self.encoder(x)\n",
    "        batch = x.shape[0]\n",
    "        x = self.trans1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.in1(x)\n",
    "        x = self.trans2(x)\n",
    "        x = self.relu2(x) \n",
    "        x = self.in2(x)\n",
    "        x = self.trans3(x)\n",
    "        x = F.sigmoid(x)  \n",
    "        x = x.transpose(1,2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_autoencoder(epochs, model, criterion, optimizer, train_dl, val_dl_test, val_dl, patience = 5):\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    val_loss_test = 0.0\n",
    "    val_loss_oof = 0.0\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    val_loss_test_list = []\n",
    "    val_loss_oof_list = []\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        for i, data in enumerate(train_dl, 0):\n",
    "            xb, yb, p = data\n",
    "            optimizer.zero_grad()\n",
    "            output = model(xb)\n",
    "            xb = xb.float()\n",
    "            loss = criterion(output, xb)/(p.sum())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()*xb.size(0)\n",
    "        model.eval()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(val_dl, 0):\n",
    "                xb, yb, p = data\n",
    "                output = model(xb)\n",
    "                xb = xb.float()\n",
    "                loss = criterion(output, xb)/(p.sum())\n",
    "                val_loss += loss.item()*xb.size(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(val_dl_test, 0):\n",
    "                xb, yb, p = data\n",
    "                output = model(xb)\n",
    "                xb = xb.float()\n",
    "                loss = criterion(output, xb)/(p.sum())\n",
    "                val_loss_test += loss.item()*xb.size(0)\n",
    "                \n",
    "        train_loss = train_loss/len(train_dl)\n",
    "        val_loss = val_loss/len(val_dl)\n",
    "        val_loss_test = val_loss_test/len(val_dl_test)\n",
    "        \n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "        val_loss_test_list.append(val_loss_test)\n",
    "        \n",
    "        print(\"Epoch :{} \\tTraining Loss :{:.6f}.\".format(epoch+1,train_loss))\n",
    "        \n",
    "        print(\"Epoch :{} \\tVal Loss :{:.6f}.\".format(epoch+1,val_loss))\n",
    "        \n",
    "        print(\"Epoch :{} \\tVal OOD Loss :{:.6f}.\".format(epoch+1,val_loss_test))\n",
    "\n",
    "        early_stopping(val_loss, model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(val_loss_list, 'b', label = \"Validation\")\n",
    "    ax.plot(train_loss_list,'r', label = \"Train\")\n",
    "    ax.plot(val_loss_test,'y', label = \"Validation Lyso\")\n",
    "    x_ticks = np.arange(0,epoch+1,1)\n",
    "    plt.xticks(x_ticks)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "# model_autoencoder = autoencoder(model).cuda()\n",
    "# criterion = nn.MSELoss(reduction=\"sum\")\n",
    "# optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model_autoencoder.parameters()), lr=1e-5)\n",
    "\n",
    "# model_A = fit_autoencoder(60, model_autoencoder, criterion, optimizer, Train_dl, Val_dl, Val_u_dl, patience=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load for inference\n",
    "\n",
    "model = pickle.load(open('../PretrainedModels/A_subcluster_0.pickle','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rerr(df, le_fam, le_fold, vocab, BATCH_SIZE=1, cuda=cuda_gpu, model, name):\n",
    "    reconstruction_err = []\n",
    "    for i, data in enumerate(Train_dl, 0):\n",
    "        model_A.eval()\n",
    "\n",
    "        xb, yb, p = data\n",
    "        output = model(xb)\n",
    "        xb = xb.float()\n",
    "        loss = criterion(output, xb)/(p.sum())\n",
    "\n",
    "        reconstruction_err.append([df.iloc[i].family, loss.item()])\n",
    "\n",
    "    return pd.DataFrame(reconstruction_err, columns=[\"fold\", \"Err\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerr_A = get_rerr(df=df_cluter, le_fam=le_fam, le_fold=le_fold, vocab=vocab, BATCH_SIZE=1, cuda=cuda_gpu, model, name)\n",
    "rerr_B = get_rerr(df=df_B, le_fam=le_fam, le_fold=le_fold, vocab=vocab, BATCH_SIZE=1, cuda=cuda_gpu, model, name)\n",
    "rerr_C = get_rerr(df=df_C, le_fam=le_fam, le_fold=le_fold, vocab=vocab, BATCH_SIZE=1, cuda=cuda_gpu, model, name)\n",
    "rerr_lyso = get_rerr(df=df_lyso, le_fam=le_fam, le_fold=le_fold, vocab=vocab, BATCH_SIZE=1, cuda=cuda_gpu, model, name)\n",
    "rerr_gtu = get_rerr(df=df_gtu, le_fam=le_fam, le_fold=le_fold, vocab=vocab, BATCH_SIZE=1, cuda=cuda_gpu, model, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerr_A.to_csv(\"rerr_gta0_training.csv\", index = False)\n",
    "alltest_rerr= pd.concat([rerr_B, rerr_C, reconstruction_err_gtc, rerr_lyso, rerr_gtu], axis=0)\n",
    "alltest_rerr.to_csv(\"rerr_gta0_Alltest.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
